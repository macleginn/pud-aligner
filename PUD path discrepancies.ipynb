{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pymorphy2\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import combinations as combs\n",
    "from queue import SimpleQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_key(k):\n",
    "    \"\"\"Converts 0-based indexing to 1-based indexing.\"\"\"\n",
    "    return str(int(k)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll2graph(record):\n",
    "    \"\"\"Converts sentences described using CoNLL-U format (http://universaldependencies.org/format.html)\n",
    "    to graphs. Returns a dictionary of nodes (wordforms and POS tags indexed by line numbers)\n",
    "    together with a graph of the dependencies encoded as adjacency lists of\n",
    "    (node_key, relation_label, direction[up or down]) tuples.\"\"\"\n",
    "    graph = {}\n",
    "    nodes = {}\n",
    "    for line in record.splitlines():\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        fields = line.strip('\\n').split('\\t')\n",
    "        key = fields[0]\n",
    "        # Ignore compound surface keys for aux, du, etc.\n",
    "        if '-' in key:\n",
    "            continue\n",
    "        # lemma would be better, but there are no lemmas in Russian PUD\n",
    "        # take care of this at a later stage\n",
    "        wordform = fields[1] \n",
    "        pos = fields[3]\n",
    "        parent = fields[6]\n",
    "        relation = fields[7]\n",
    "        nodes[key] = { 'wordform': wordform, 'pos': pos }\n",
    "        if key not in graph:\n",
    "            graph[key] = []\n",
    "        if parent not in graph:\n",
    "            graph[parent] = []\n",
    "        graph[key].append((parent, relation, 'up'))\n",
    "        graph[parent].append((key, relation, 'down'))\n",
    "    return (nodes, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_sentences(record):\n",
    "    \"\"\"Extracts target and source sentences from the target record.\"\"\"\n",
    "    lines = record[3].splitlines()\n",
    "    for l in lines:\n",
    "        if l.startswith('# text = '):\n",
    "            target = l.strip('\\n')[len('# text = '):]\n",
    "            for l2 in lines:\n",
    "                if l2.startswith('# text_en = '):\n",
    "                    source = l2.strip('\\n')[len('# text_en = '):]\n",
    "                    return (source, target)\n",
    "            else:\n",
    "                raise ValueError('No source sentence found')\n",
    "    else:\n",
    "        raise ValueError('No target sentence found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_alignment(alignment_str):\n",
    "    \"\"\"Extracts unaligned words and one-to-many alignments.\n",
    "    returns remaining edges as a list.\"\"\"\n",
    "    en_degrees = Counter()\n",
    "    fr_degrees = Counter()\n",
    "    unaligned_en = []\n",
    "    unaligned_fr = []\n",
    "    one_to_many_en = {}\n",
    "    one_to_many_fr = {}\n",
    "    alignment_edges = alignment_str.split()\n",
    "    real_edges = []\n",
    "    resulting_edges = []\n",
    "    for edge in alignment_edges:\n",
    "        en, fr = edge.split('-')\n",
    "        if en == 'X':\n",
    "            unaligned_fr.append(fr)\n",
    "        elif fr == 'X':\n",
    "            unaligned_en.append(en)\n",
    "        else:\n",
    "            en_degrees[en] += 1\n",
    "            fr_degrees[fr] += 1\n",
    "            real_edges.append((en, fr))\n",
    "    for edge in real_edges:\n",
    "        en, fr = edge\n",
    "        if en_degrees[en] > 1:\n",
    "            if en not in one_to_many_en:\n",
    "                one_to_many_en[en] = []\n",
    "            one_to_many_en[en].append(fr)\n",
    "        elif fr_degrees[fr] > 1:\n",
    "            if fr not in one_to_many_fr:\n",
    "                one_to_many_fr[fr] = []\n",
    "            one_to_many_fr[fr].append(en)\n",
    "        else:\n",
    "            resulting_edges.append(edge)\n",
    "    return (\n",
    "        unaligned_en,\n",
    "        unaligned_fr,\n",
    "        one_to_many_en,\n",
    "        one_to_many_fr,\n",
    "        resulting_edges\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(node1, node2, graph):\n",
    "    if node1 == node2:\n",
    "        return []\n",
    "    \n",
    "    # BFS with edge labels for paths\n",
    "    q = SimpleQueue()\n",
    "    # Remembers where we came from and the edge label\n",
    "    sources = {}\n",
    "    \n",
    "    q.put(node1)\n",
    "    visited = set()\n",
    "    visited.add(node1)\n",
    "    \n",
    "    while not q.empty():\n",
    "        current = q.get()\n",
    "        for neighbour, relation, direction in graph[current]:\n",
    "            if neighbour == node2:\n",
    "                path = [relation+'_'+direction]\n",
    "                source = current\n",
    "                while source != node1:\n",
    "                    prev_node, prev_relation, prev_direction = sources[source]\n",
    "                    path.append(prev_relation+'_'+prev_direction)\n",
    "                    source = prev_node\n",
    "                return list(reversed(path))\n",
    "            elif neighbour not in visited:\n",
    "                sources[neighbour] = (current, relation, direction)\n",
    "                q.put(neighbour)\n",
    "            visited.add(neighbour)\n",
    "            \n",
    "    raise ValueError(\"UD graph is not connected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_directions(path):\n",
    "    return list(map(lambda x: x.split('_')[0], path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_depth(node, graph):\n",
    "    # BFS\n",
    "    cur_depth = 0\n",
    "    q = SimpleQueue()\n",
    "    q.put(('0',0))\n",
    "    visited = set()\n",
    "    visited.add('0')\n",
    "    while not q.empty():\n",
    "        current_node, current_depth = q.get()\n",
    "        for neighbour, *_ in graph[current_node]:\n",
    "            if neighbour == node:\n",
    "                return current_depth+1\n",
    "            elif neighbour not in visited:\n",
    "                q.put((neighbour, current_depth+1))\n",
    "            visited.add(neighbour)\n",
    "    raise IndexError(\"Target node unreachable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('pud_30_12.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ru = [r for r in cursor.execute('select * from `en-ru`')][:500]\n",
    "en_fr = [r for r in cursor.execute('select * from `en-fr`')][:776]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_n, fr_g = conll2graph(en_fr[35][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'wordform': 'The', 'pos': 'DET'},\n",
       " '2': {'wordform': 'company', 'pos': 'NOUN'},\n",
       " '3': {'wordform': 'told', 'pos': 'VERB'},\n",
       " '4': {'wordform': 'the', 'pos': 'DET'},\n",
       " '5': {'wordform': 'BBC', 'pos': 'PROPN'},\n",
       " '6': {'wordform': 'it', 'pos': 'PRON'},\n",
       " '7': {'wordform': 'would', 'pos': 'AUX'},\n",
       " '8': {'wordform': 'be', 'pos': 'AUX'},\n",
       " '9': {'wordform': 'the', 'pos': 'DET'},\n",
       " '10': {'wordform': 'responsibility', 'pos': 'NOUN'},\n",
       " '11': {'wordform': 'of', 'pos': 'ADP'},\n",
       " '12': {'wordform': 'each', 'pos': 'DET'},\n",
       " '13': {'wordform': 'airline', 'pos': 'NOUN'},\n",
       " '14': {'wordform': 'brand', 'pos': 'NOUN'},\n",
       " '15': {'wordform': 'to', 'pos': 'PART'},\n",
       " '16': {'wordform': 'decide', 'pos': 'VERB'},\n",
       " '17': {'wordform': 'whether', 'pos': 'SCONJ'},\n",
       " '18': {'wordform': 'to', 'pos': 'PART'},\n",
       " '19': {'wordform': 'charge', 'pos': 'VERB'},\n",
       " '20': {'wordform': 'passengers', 'pos': 'NOUN'},\n",
       " '21': {'wordform': 'an', 'pos': 'DET'},\n",
       " '22': {'wordform': 'access', 'pos': 'NOUN'},\n",
       " '23': {'wordform': 'fee', 'pos': 'NOUN'},\n",
       " '24': {'wordform': '.', 'pos': 'PUNCT'}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_node_depth('16', fr_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "The judge in Duffy's fraud and breach of trust trial ultimately ruled they were within the Senate's rules when he cleared Duffy of all charges. [Duffy->charges] ['obj_up', 'obl_down']\n",
      "Судья в процессе по делу Даффи (Duffy) о мошенничестве и злоупотреблении доверием в конечном счете принял решение, что таковые имели место в пределах правил Сената, сняв при этом с Даффи все обвинения. [Даффи->обвинения] ['obl_up', 'obj_down']\n",
      "\n",
      "260\n",
      "News of the company’s deceit - which had run for years - wiped tens of billions of euros from VW's value and cost chief executive Martin Winterkorn his job. [value->tens] ['obl_up', 'obj_down']\n",
      "Новости о мошенничестве компании были на слуху уже несколько лет, тем самым понизив стоимость «Фольксвагена» на десятки миллиардов евро и лишив главного исполнительного директора Мартина Винтеркорна его должности. [стоимость->десятки] ['obj_up', 'obl_down']\n",
      "\n",
      "275\n",
      "This means that they have not benefited from the uplift that the fall in sterling has given to overseas assets. [fall->assets] ['nsubj_up', 'obl_down']\n",
      "Это значит, что они не воспользовались ростом, который получили зарубежные активы из-за падения британского фунта. [падения->активы] ['obl_up', 'nsubj_down']\n",
      "\n",
      "420\n",
      "It provided routes for trade, colonisation, and war, as well as food (from fishing and the gathering of other seafood) for numerous communities throughout the ages. [trade->food] ['nmod_up', 'conj_down']\n",
      "Он обеспечивал маршруты для торговли, войны, а также добычи продовольствия (за счет рыболовства и сбора других морепродуктов) для многочисленных общин на протяжении веков. [торговли->продовольствия] ['conj_down', 'nmod_down']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing code\n",
    "# n = 44\n",
    "# en_n, en_g = conll2graph(en_fr[n][2])\n",
    "# fr_n, fr_g = conll2graph(en_fr[n][3])\n",
    "# (unaligned_en, unalined_fr, one_to_many_en, one_to_many_fr, \n",
    "#      alignment_edges) = preprocess_alignment(en_fr[n][4])\n",
    "# print(len(alignment_edges))\n",
    "# for en, fr in sorted(alignment_edges, key = lambda x: int(x[0])):\n",
    "#     print(en_n[normalise_key(en)]['wordform'] + '->' + fr_n[normalise_key(fr)]['wordform'])\n",
    "# print()\n",
    "\n",
    "for i, record in enumerate(en_ru):\n",
    "    en_n, en_g = conll2graph(record[2])\n",
    "    fr_n, fr_g = conll2graph(record[3])\n",
    "    (unaligned_en, unalined_fr, one_to_many_en, one_to_many_fr, \n",
    "     alignment_edges) = preprocess_alignment(record[4])\n",
    "    source_sent, target_sent = extract_raw_sentences(record)\n",
    "    # Extract highest-positioned counterparts from one-to-many alignments\n",
    "    for node_en, nodes_fr in one_to_many_en.items():\n",
    "        min_depth = 1000\n",
    "        arg_min = 'X'\n",
    "        for n_fr in nodes_fr:\n",
    "            current_depth = get_node_depth(normalise_key(n_fr), fr_g)\n",
    "            if current_depth < min_depth:\n",
    "                min_depth = current_depth\n",
    "                arg_min = n_fr\n",
    "        if arg_min == 'X':\n",
    "            raise ValueError(\"Minimum-depth node not found\")\n",
    "        alignment_edges.append((node_en, arg_min))\n",
    "    for node_fr, nodes_en in one_to_many_fr.items():\n",
    "        min_depth = 1000\n",
    "        arg_min = 'X'\n",
    "        for n_en in nodes_en:\n",
    "            current_depth = get_node_depth(normalise_key(n_en), en_g)\n",
    "            if current_depth < min_depth:\n",
    "                min_depth = current_depth\n",
    "                arg_min = n_en\n",
    "        if arg_min == 'X':\n",
    "            raise ValueError(\"Minimum-depth node not found\")\n",
    "        alignment_edges.append((arg_min, node_fr))\n",
    "    for c in combs(alignment_edges, 2):\n",
    "        p, q = c\n",
    "        en1, fr1 = map(normalise_key, p)\n",
    "        en2, fr2 = map(normalise_key, q)\n",
    "        if fr_n[fr1]['pos'] == 'CCONJ' or fr_n[fr2]['pos'] == 'CCONJ':\n",
    "            continue # CCONJs were not aligned for Russian\n",
    "        path_en = get_path(en1, en2, en_g)\n",
    "        path_fr = get_path(fr1, fr2, fr_g)\n",
    "        if len(path_en) == 2:\n",
    "            path_en_stripped = strip_directions(path_en)\n",
    "            path_fr_stripped = strip_directions(path_fr)\n",
    "            if path_en_stripped != path_fr_stripped and path_en_stripped == list(reversed(path_fr_stripped)):\n",
    "                print(i+1)\n",
    "                print(source_sent, f'[{en_n[en1][\"wordform\"]}->{en_n[en2][\"wordform\"]}]', path_en)\n",
    "                print(target_sent, f'[{fr_n[fr1][\"wordform\"]}->{fr_n[fr2][\"wordform\"]}]', path_fr)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
